###album catalog: cellcanvas

from album.runner.api import setup, get_data_path, get_args

env_file = """name: segmentation-env
channels:
  - conda-forge
  - defaults
dependencies:
  - python=3.10
  - scikit-learn==1.3.2
  - joblib
  - numpy
  - zarr
  - pip
"""

def run():
    import joblib
    import numpy as np
    import zarr

    def load_model(model_path):
        """Load the random forest model from a joblib file."""
        return joblib.load(model_path)

    def apply_model_to_embeddings_in_chunks(embeddings_path, model, output_path):
        """Load embeddings from Zarr in chunks, apply the model, and correctly handle transposed predictions."""
        zarr_embeddings = zarr.open(embeddings_path, mode='r')
        # Note: The shape for the output Zarr should consider the transposition of the last two dimensions of the input
        output_shape = (zarr_embeddings.shape[2], zarr_embeddings.shape[1], zarr_embeddings.shape[0])
        output_zarr = zarr.open(output_path, shape=output_shape, chunks=True, dtype=int, mode='w')

        chunk_size = 200  # Define your chunk size

        # Iterate over the Zarr dataset in chunks
        for x in range(0, zarr_embeddings.shape[0], chunk_size):
            for y in range(0, zarr_embeddings.shape[1], chunk_size):
                for z in range(0, zarr_embeddings.shape[2], chunk_size):
                    # Define slice for the current chunk with correct dimensions
                    input_slice = (slice(x, min(x + chunk_size, zarr_embeddings.shape[0])),
                                   slice(y, min(y + chunk_size, zarr_embeddings.shape[1])),
                                   slice(z, min(z + chunk_size, zarr_embeddings.shape[2])))

                    chunk = zarr_embeddings[input_slice]
                    chunk_reshaped = chunk.reshape(-1, chunk.shape[-1])
                    predictions = model.predict(chunk_reshaped)

                    # Reshape predictions to match the input chunk's shape
                    predictions_reshaped = predictions.reshape(chunk.shape[:-1])

                    # Transpose operation to align with the full array's transpose requirement
                    predictions_transposed = np.transpose(predictions_reshaped, (2, 1, 0))

                    # Calculate the output slice considering the transposed dimensions
                    output_slice = (slice(z, min(z + chunk_size, output_shape[0])),
                                    slice(y, min(y + chunk_size, output_shape[1])),
                                    slice(x, min(x + chunk_size, output_shape[2])))

                    output_zarr[output_slice] = predictions_transposed

    # Paths and model loading
    embeddings_path = get_args().zarrembedding
    model_path = get_args().modelpath
    output_path = get_args().zarroutput

    model = load_model(model_path)
    apply_model_to_embeddings_in_chunks(embeddings_path, model, output_path)

    print(f"Segmentation output saved to {output_path}")


setup(
    group="cellcanvas",
    name="segment-tomogram",
    version="0.0.4",
    title="Segmentation using Random Forest on Embeddings in Chunks",
    description="Apply a Random Forest model to embeddings generated by TomoTwin to produce segmentation output, processing in chunks.",
    solution_creators=["Kyle Harrington"],
    cite=[{"text": "CellCanvas team.", "url": "https://cellcanvas.org"}],
    tags=["segmentation", "random forest", "machine learning", "cryoet", "chunks"],
    license="MIT",
    covers=[],
    album_api_version="0.5.1",
    args=[
        {"name": "zarrembedding", "type": "string", "required": True, "description": "Path to the input Zarr file containing embeddings"},
        {"name": "zarroutput", "type": "string", "required": True, "description": "Path for the output Zarr file containing segmentation"},
        {"name": "modelpath", "type": "string", "required": True, "description": "Path to the joblib file containing the trained Random Forest model"},
    ],
    run=run,
    dependencies={
        "environment_file": env_file
    },
)
