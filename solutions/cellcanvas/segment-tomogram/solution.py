###album catalog: cellcanvas

from album.runner.api import setup, get_data_path, get_args

env_file = """name: segmentation-env
channels:
  - conda-forge
  - defaults
dependencies:
  - python=3.10
  - scikit-learn==1.3.2
  - joblib
  - numpy
  - zarr
  - pip
"""

def run():
    import os
    import joblib
    import numpy as np
    import zarr

    def load_model(model_path):
        """Load the random forest model from a joblib file."""
        return joblib.load(model_path)

    def apply_model_to_embeddings_in_chunks(embeddings_path, model, output_path, chunk_size=200):
        """Load embeddings from Zarr in chunks, apply the model, and save transposed predictions to a new Zarr file."""
        zarr_embeddings = zarr.open(embeddings_path, mode='r')
        output_shape = zarr_embeddings.shape[:-1]  # Exclude the last dimension (features) for the output shape
        output_zarr = zarr.open(output_path, shape=output_shape, dtype=int, chunks=(chunk_size, chunk_size, chunk_size), mode='w')

        # Determine the number of chunks needed along each dimension
        for x_start in range(0, output_shape[0], chunk_size):
            for y_start in range(0, output_shape[1], chunk_size):
                for z_start in range(0, output_shape[2], chunk_size):
                    x_end = min(x_start + chunk_size, output_shape[0])
                    y_end = min(y_start + chunk_size, output_shape[1])
                    z_end = min(z_start + chunk_size, output_shape[2])

                    # Extract the current chunk from the embeddings
                    chunk = zarr_embeddings[x_start:x_end, y_start:y_end, z_start:z_end]
                    chunk_reshaped = chunk.reshape(-1, chunk.shape[-1])  # Reshape for prediction
                    predictions = model.predict(chunk_reshaped).reshape(chunk.shape[:-1])  # Predict and reshape back

                    # Transpose the predictions if necessary to match the expected spatial orientation
                    predictions_transposed = np.transpose(predictions, (2, 1, 0))  # Adjust this based on your data's orientation
                    
                    # Write the transposed predictions back to the corresponding location in the output Zarr array
                    output_zarr[x_start:x_end, y_start:y_end, z_start:z_end] = predictions_transposed

    # Paths and model loading
    embeddings_path = get_args().zarrembedding
    model_path = get_args().modelpath
    output_path = get_args().zarroutput

    model = load_model(model_path)
    apply_model_to_embeddings_in_chunks(embeddings_path, model, output_path)

    print(f"Segmentation output saved to {output_path}")


setup(
    group="cellcanvas",
    name="segment-tomogram",
    version="0.0.4",
    title="Segmentation using Random Forest on Embeddings in Chunks",
    description="Apply a Random Forest model to embeddings generated by TomoTwin to produce segmentation output, processing in chunks.",
    solution_creators=["Kyle Harrington"],
    cite=[{"text": "CellCanvas team.", "url": "https://cellcanvas.org"}],
    tags=["segmentation", "random forest", "machine learning", "cryoet", "chunks"],
    license="MIT",
    covers=[],
    album_api_version="0.5.1",
    args=[
        {"name": "zarrembedding", "type": "string", "required": True, "description": "Path to the input Zarr file containing embeddings"},
        {"name": "zarroutput", "type": "string", "required": True, "description": "Path for the output Zarr file containing segmentation"},
        {"name": "modelpath", "type": "string", "required": True, "description": "Path to the joblib file containing the trained Random Forest model"},
    ],
    run=run,
    dependencies={
        "environment_file": env_file
    },
)
